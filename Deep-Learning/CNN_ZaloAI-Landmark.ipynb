{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6DN3gs3aT6Mg",
        "Oj4C8OMZT6Mh",
        "qO-PkgqyT6Mj",
        "CkbJdSYmT6Mk"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuiKELEdT6MF"
      },
      "source": [
        "# Giới thiệu Convolution Nets\n",
        "\n",
        "Convolutional Neural Networks (CNN) là một trong những mô hình deep learning phổ biến nhất và có ảnh hưởng nhiều nhất trong cộng đồng Computer Vision. CNN được dùng trong trong nhiều bài toán như nhận dạng ảnh, phân tích video, ảnh MRI, hoặc cho bài các bài của lĩnh vực xử lý ngôn ngữ tự nhiên, và hầu hết đều giải quyết tốt các bài toán này. \n",
        "\n",
        "CNN cũng có lịch sử khá lâu đời. Kiến trúc gốc của mô hình CNN được giới thiệu bởi một nhà khoa học máy tính người Nhật vào năm 1980. Sau đó, năm 1998, Yan LeCun lần đầu huấn luyện mô hình CNN với thuật toán backpropagation cho bài toán nhận dạng chữ viết tay. Tuy nhiên, mãi đến năm 2012, khi một nhà khoa học máy tính người Ukraine Alex Krizhevsky (đệ của Geoffrey Hinton) xây dựng mô hình CNN (AlexNet) và sử dụng GPU để tăng tốc quá trình huấn luyện deep nets để đạt được top 1 trong cuộc thi Computer Vision thường niên ImageNet với độ lỗi phân lớp top 5 giảm hơn 10% so với những mô hình truyền thống trước đó, đã tạo nên làn sóng mãnh mẽ sử dụng deep CNN với sự hỗ trợ của GPU để giải quyết càng nhiều các vấn đề trong Computer Vision.\n",
        "\n",
        "# Bài Toán Phân loại Ảnh\n",
        "Phân loại ảnh là một bài toán quan trọng bậc nhất trong lĩnh vực Computer Vision. Chúng ta đã có rất nhiều nghiên cứu để giải quyết bài toán này bằng cách rút trích các đặc trưng rất phổ biến như SIFT, HOG rồi cho máy tính học nhưng những cách này tỏ ra không thực sự hiểu quả. Nhưng ngược lại, đối với con người, chúng ta lại có bản năng tuyệt vời để phân loại được những đối tượng trong khung cảnh xung quanh một cách dễ dàng.\n",
        "\n",
        "Dữ liệu đầu vào của bài toán là một bức ảnh. Một ảnh được biểu diễn bằng ma trận các giá trị. Mô hình phân lớp sẽ phải dự đoán được lớp của ảnh từ ma trận điểm ảnh này, ví dụ như ảnh đó là con mèo, chó, hay là chim.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/cnn_input.png)\n",
        "\n",
        "# Nội dung \n",
        "\n",
        "1. Import/ Xử lý dữ liệu\n",
        "2. Xây dựng mô hình\n",
        "3. Huấn luyện mô hình\n",
        "4. Đánh giá mô hình\n",
        "5. Sử dụng mô hình đã huấn luyện để dự đoán"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1noDSEH2T6MG"
      },
      "source": [
        "# Import thư viện\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW-ZLnnrjTly",
        "outputId": "ead2ffea-435b-477f-e565-8499dff74375"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce3fbIErjZFR",
        "outputId": "59ba28a2-a74c-403b-c9f5-7afd4a933978"
      },
      "source": [
        "cd 'drive/MyDrive'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpEf76pvT6MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eacaae3-99ce-4f2c-dde5-3ac190f598d5"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "np.warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "print(\"Tensorflow version: \", tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version:  2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-drPZYqT6MK"
      },
      "source": [
        "# Import và inspect dữ liệu\n",
        "Tập dữ liệu huấn luyện bao gồm 10k ảnh, là một phần nhỏ của bộ dữ liệu trong cuộc thi ZaloAI năm 2018. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OgiAISfDBVD",
        "outputId": "edadd202-77d2-45a3-f0b4-1d79cab6980c"
      },
      "source": [
        "data_dir = 'data'\n",
        "os.listdir(data_dir)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['images', 'train.csv', 'sample_submission.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkHAl5onDQjs"
      },
      "source": [
        "Trong đó:\n",
        "- **images**: thư mục chứa tất cả các ảnh dùng cho việc huấn luyện và đánh giá\n",
        "- **train.csv**: file CSV chứa tên các file và nhãn dùng cho việc huấn luyện\n",
        "- **sample_submission.csv**: file CSV mẫu chứa tên các file cần đánh giá và nhãn dummy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg3f7Xn6EFC-"
      },
      "source": [
        "## Đọc và xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLFGYbfMWDQ7"
      },
      "source": [
        "Đọc dữ liệu từ file CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "MH__gPbQJSLV",
        "outputId": "428473ed-b5a4-4c8d-caa3-fe1094440632"
      },
      "source": [
        "train_df = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "train_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VietAI-Assignment3-1.jpg</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VietAI-Assignment3-100.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VietAI-Assignment3-10000.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VietAI-Assignment3-10001.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VietAI-Assignment3-10002.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          image  label\n",
              "0      VietAI-Assignment3-1.jpg      7\n",
              "1    VietAI-Assignment3-100.jpg      2\n",
              "2  VietAI-Assignment3-10000.jpg      1\n",
              "3  VietAI-Assignment3-10001.jpg      2\n",
              "4  VietAI-Assignment3-10002.jpg      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBRrzcibWMWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1405fa7-740e-4925-b29a-2969e93cc013"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8234 entries, 0 to 8233\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   image   8234 non-null   object\n",
            " 1   label   8234 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 128.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "syh-kjLNbiQd",
        "outputId": "7a43469a-94e5-40de-d4ea-a6845f64ffbb"
      },
      "source": [
        "test_df = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n",
        "test_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>VietAI-Assignment3-10.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VietAI-Assignment3-1000.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VietAI-Assignment3-10004.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VietAI-Assignment3-10006.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VietAI-Assignment3-10012.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          image  label\n",
              "0     VietAI-Assignment3-10.jpg      0\n",
              "1   VietAI-Assignment3-1000.jpg      0\n",
              "2  VietAI-Assignment3-10004.jpg      0\n",
              "3  VietAI-Assignment3-10006.jpg      0\n",
              "4  VietAI-Assignment3-10012.jpg      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzBkL72Sbpg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf1c87e-b421-4aef-d18e-a01dcf7c96c9"
      },
      "source": [
        "test_df.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2059 entries, 0 to 2058\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   image   2059 non-null   object\n",
            " 1   label   2059 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 32.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buXAweOvENiO"
      },
      "source": [
        "Tổng cộng có 8234 ảnh cho việc huấn luyện và 2059 ảnh cần dự đoán nhãn, ta tiến hành thống kê phân bố các nhãn trên tập huấn luyện:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZPoB7-gV8C6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4dc8987-1fde-43bd-ca92-c62b225efae1"
      },
      "source": [
        "train_df.label.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2     1949\n",
              "1      846\n",
              "0      808\n",
              "3      764\n",
              "5      642\n",
              "4      599\n",
              "7      579\n",
              "10     575\n",
              "6      535\n",
              "8      469\n",
              "9      468\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo0z1gUIVcmj"
      },
      "source": [
        "Số lượng các ảnh cho mỗi lớp từ 400 đến 2000. Trong đó lớp số 2 có số lượng ảnh nhiều nhất"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIHjhdzRZ8Z3"
      },
      "source": [
        "def generate_data(image_paths, size=224):\n",
        "    \"\"\"\n",
        "    Đọc và chuyển các ảnh về numpy array\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    image_paths: list of N strings\n",
        "        List các đường dẫn ảnh\n",
        "    size: int\n",
        "        Kích thước ảnh cần resize\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    numpy array kích thước (N, size, size, 3)\n",
        "    \"\"\"\n",
        "    image_array = np.zeros((len(image_paths), size, size, 3), dtype='float32')\n",
        "    \n",
        "    for idx, image_path in tqdm(enumerate(image_paths)):\n",
        "        ### START CODE HERE\n",
        "        \n",
        "        # Đọc ảnh bằng thư viện Pillow và resize ảnh\n",
        "        image = Image.open(image_path).convert(\"RGB\").resize((size,size))\n",
        "        \n",
        "        # Chuyển ảnh thành numpy array và gán lại mảng image_array\n",
        "        image_array[idx] = np.array(image).astype('float32')/255\n",
        "        \n",
        "        ### END CODE HERE\n",
        "    return image_array"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9sI7QBibZdG"
      },
      "source": [
        "Sử dụng hàm `generate_data` để tạo ma trận của tập dữ liệu train và test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7UkA-_rbYzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafd0c3f-afd6-4588-c0df-807934845adc"
      },
      "source": [
        "# List các đường dẫn file cho việc huấn luyện\n",
        "train_files = [os.path.join(\"data/images\", file) for file in train_df.image]\n",
        "\n",
        "# List các nhãn\n",
        "train_y = train_df.label\n",
        "\n",
        "# Tạo numpy array cho dữ liệu huấn luyện\n",
        "train_arr = generate_data(train_files)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8234it [29:23,  4.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxScb0w_e2Gh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ff79ad-ba8c-4641-c3d4-93940082e511"
      },
      "source": [
        "train_arr.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8234, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msp9WEgOfRdf"
      },
      "source": [
        "Tiến hành tạo tensor dữ liệu cho tập test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vI-AlLgfXi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013646f0-6a03-4050-a3c6-ab268d5ec3e7"
      },
      "source": [
        "test_files = [os.path.join(\"data/images\", file) for file in test_df.image]\n",
        "test_x = generate_data(test_files)\n",
        "test_x.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2059it [07:11,  4.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2059, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LssbDy2pf9ua"
      },
      "source": [
        "Tạo **one-hot labels** từ `train_y` để đưa vào huấn luyện với Tensorflow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVuRanKET6MO"
      },
      "source": [
        "num_classes = len(np.unique(train_y))\n",
        "y_one = tf.keras.utils.to_categorical(train_y, num_classes=num_classes)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjpnYIXRT6MT"
      },
      "source": [
        "## Chia dữ liệu để huấn luyện và đánh giá\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtnwiHOZT6MU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524ee065-66e7-4573-bbd3-b3e2fd2b241b"
      },
      "source": [
        "x_train, x_valid, y_train_one, y_valid_one = train_test_split(train_arr, y_one, test_size=0.25)\n",
        "\n",
        "print(\"Train size: {} - Validation size: {}\".format(x_train.shape, x_valid.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size: (6175, 224, 224, 3) - Validation size: (2059, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCTUEY2dBIUQ"
      },
      "source": [
        "new_train_df, val_df = train_test_split(train_df, test_size=0.2)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06g6e_u7T6MY"
      },
      "source": [
        "## Mô Hình CNN\n",
        "\n",
        "CNN bao gồm tập hợp các lớp cơ bản bao sau: convolution layer + nonlinear layer (RELU, ...), pooling layer, fully connected layer. Các lớp này liên kết với nhau theo một thứ tự nhất định. Thông thường, một ảnh sẽ được lan truyền qua tầng convolution layer + nonlinear layer đầu tiên, sau đó các giá trị tính toán được sẽ lan truyền qua pooling layer, bộ ba convolution layer + nonlinear layer + pooling layer có thể được lặp lại nhiều lần trong network. Và sau đó được lan truyền qua tầng fully connected layer và softmax để tính xác suất ảnh đó chứa vật thế gì.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/cnn_model.png)\n",
        "\n",
        "### Convolution Layer\n",
        "Convolution layer là lớp đầu tiên và cũng là lớp quan trọng nhất của mô hình CNN. Lớp này có chức năng chính là phát hiện các đặc trưng về không gian một cách hiệu quả. Trong tầng này có 4 đối tượng chính là: ma trận đầu vào, bộ **filter**, và **receptive field**, **feature map**. Conv layer nhận đầu vào là một ma trận 3 chiều và một bộ filter cần phải học. Bộ filters này sẽ trượt qua từng vị trí trên bức ảnh để tính tích chập (convolution) giữa bộ filter và phần tương ứng trên bức ảnh. Phần tương ứng này trên bức ảnh gọi là receptive field, tức là vùng mà một neuron có thể nhìn thấy để đưa ra quyết định, và mà trận cho ra bởi quá trình này được gọi là feature map. Để hình dung, các bạn có thể tưởng tượng, bộ filters giống như các tháp canh trong nhà tù quét lần lượt qua không gian xung quanh để tìm kiếm tên tù nhân bỏ trốn. Khi phát hiện tên tù nhân bỏ trốn, thì chuông báo động sẽ reo lên, giống như các bộ filters tìm kiếm được đặc trưng nhất định thì tích chập đó sẽ cho giá trị lớn. \n",
        "\n",
        "<div class=\"img-div\" markdown=\"0\">\n",
        "    <img src=\"https://media.giphy.com/media/3orif7it9f4phjv4LS/giphy.gif\" />\n",
        "</div>\n",
        "\n",
        "Với ví dụ ở bên dưới, dữ liệu đầu vào ở là ma trận có kích thước 8x8x1, một bộ filter có kích thước 2x2x1, feature map có kích thước 7x7x1. Mỗi giá trị ở feature map được tính bằng tổng của tích các phần tử tương ứng của bộ filter 2x2x1 với receptive field trên ảnh. Và để tính tất cả các giá trị cho feature map, các bạn cần trượt filter từ trái sang phải, từ trên xuống dưới. Do đó, các bạn có thể thấy rằng phép convolution bảo toàn thứ tự không gian của các điểm ảnh. Ví dụ điểm góc trái của dữ liệu đầu vào sẽ tương ứng với bên một điểm bên góc trái của feature map. \n",
        "\n",
        "<div class=\"img-div\" markdown=\"0\">\n",
        "    <img src=\"https://pbcquoc.github.io/images/cnn_covolution_layer.png\" />\n",
        "</div>\n",
        "\n",
        "#### Tầng convolution như là feature detector \n",
        "\n",
        "Tầng convolution có chức năng chính là phát hiện đặc trưng cụ thể của bức ảnh. Những đặc trưng này bao gồm đặc trưng cơ bản là góc, cạnh, màu sắc, hoặc đặc trưng phức tạp hơn như texture của ảnh. Vì bộ filter quét qua toàn bộ bức ảnh, nên những đặc trưng này có thể nằm ở vị trí bất kì trong bức ảnh, cho dù ảnh bị xoay trái/phải thì những đặc trưng này vẫn bị phát hiện. \n",
        "\n",
        "Ở minh họa dưới, các bạn có một filter 5x5 dùng để phát hiện góc/cạnh, filter này chỉ có giá trị một tại các điểm tương ứng một góc cong. \n",
        "\n",
        "<div class=\"img-div\" markdown=\"0\">\n",
        "    <img src=\"https://pbcquoc.github.io/images/cnn_high_level_feature.png\" />\n",
        "</div>\n",
        "\n",
        "Dùng filter ở trên trượt qua ảnh của nhân vật Olaf trong trong bộ phim Frozen. Chúng ta thấy rằng, chỉ ở những vị trí trên bức ảnh có dạng góc như đặc trưng ở filter thì mới có giá trị lớn trên feature map, những vị trí còn lại sẽ cho giá trị thấp hơn. Điều này có nghĩa là, filter đã phát hiện thành công một dạng góc/cạnh trên dự liệu đầu vào. Tập hợp nhiều bộ filters sẽ cho phép các bạn phát hiện được nhiều loại đặc trưng khác nhau, và giúp định danh được đối tượng. \n",
        "\n",
        "<div class=\"img-div\" markdown=\"0\">\n",
        "    <img src=\"https://pbcquoc.github.io/images/cnn_high_level_feature_ex.png\" />\n",
        "</div>\n",
        "\n",
        "#### Các tham số của tầng convolution: Kích thước bộ filter, stride và padding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CATPodwfT6MZ"
      },
      "source": [
        "# Xây dựng mô hình\n",
        "\n",
        "![](https://github.com/pbcquoc/cnn/raw/master/images/cnn_architecture_2.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkT5hTxjT6Mb"
      },
      "source": [
        "class ConvBlock(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel, strides, padding):\n",
        "        '''\n",
        "        Khởi tạo Convolution Block với các tham số đầu vào\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        filters: int\n",
        "            số lượng filter\n",
        "        kernel: int\n",
        "            kích thước kernel\n",
        "        strides: int\n",
        "            stride của convolution layer\n",
        "        padding: str\n",
        "            Loại padding của convolution layer\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        super(ConvBlock, self).__init__()\n",
        " \n",
        "        \n",
        "        # Tạo layer Conv2D\n",
        "        self.cnn = tf.keras.layers.Conv2D(filters, (kernel, kernel), strides=(strides, strides), kernel_initializer='he_normal', padding=padding)\n",
        "        \n",
        "        # Tạo layer MaxPool2D\n",
        "        self.pool = tf.keras.layers.MaxPool2D((2,2), strides=(2,2))\n",
        "        \n",
        "        # Tạo các layer khác tùy ý nếu cần thiết\n",
        "        self.bn = tf.keras.layers.BatchNormalization()\n",
        "        \n",
        "     \n",
        "        \n",
        "        \n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        Hàm này sẽ được gọi trong quá trình forwarding của mạng\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs: tensor đầu vào\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        tensor\n",
        "            giá trị đầu ra của mạng\n",
        "        '''\n",
        "        \n",
        "        x = None\n",
        "\n",
        "        \n",
        "        # Forward inputs qua từng layer và gán vào biến x để trả về\n",
        "        \n",
        "        x = self.cnn(inputs)\n",
        "        x = self.bn(x)\n",
        "        x = tf.nn.relu(x)   \n",
        "        x = self.pool(x)\n",
        "        \n",
        "        ## END CODE HERE\n",
        "\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1avJ7wvxpFq"
      },
      "source": [
        "## Định nghĩa toàn bộ mô hình CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn6w7oh-T6Md"
      },
      "source": [
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        # Khởi tạo các convolution block\n",
        "        self.block1 = ConvBlock(64, kernel=3, strides=1, padding='same')\n",
        "        self.block2 = ConvBlock(128, kernel=3, strides=1, padding='same')\n",
        "        self.block3 = ConvBlock(256, kernel=3, strides=1, padding='same')\n",
        "        self.block4 = ConvBlock(512, kernel=3, strides=1, padding='same')\n",
        "        self.block5 = ConvBlock(512, kernel=3, strides=1, padding='same')       \n",
        "        self.block6 = ConvBlock(1024, kernel=3, strides=1, padding='same')\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
        "        \n",
        "        # Khởi tạo layer để flatten feature map \n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        \n",
        "        ### END CODE HERE\n",
        "        \n",
        "        # Khởi tạo fully connected layer\n",
        "        self.dense1 = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        # Forward gía trị inputs qua các tầng CNN và gán vào x\n",
        "        x = self.block1(inputs)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        \n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        \n",
        "       \n",
        "        \n",
        "        # Forward giá trị x qua Fully connected layer\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        ### END CODE HERE\n",
        "        \n",
        "        # Để sử dụng hàm softmax, ta phải thực thi trên CPU\n",
        "        with tf.device('/cpu:0'):\n",
        "            output = tf.nn.softmax(x)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeV9Ab03T6Mk"
      },
      "source": [
        "# Huấn Luyện\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmZonE2T6Ml"
      },
      "source": [
        "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
        "batch_size = 32\n",
        "epochs = 16\n",
        "\n",
        "with tf.device(device):\n",
        "    # Khởi tạo model\n",
        "    model = CNN(num_classes)\n",
        "    \n",
        "    # Tạo callback để lưu model có accuracy trên tập validation tốt nhất\n",
        "    mcp = tf.keras.callbacks.ModelCheckpoint(\"my_model.h5\", monitor=\"val_acc\",\n",
        "                      save_best_only=True, save_weights_only=True)\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Huấn luyện\n",
        "    print(y_train_one)\n",
        "    model.fit(x_train, y_train_one, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(x_valid, y_valid_one), verbose=1, callbacks=[mcp])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p4QgrUJT6Mp"
      },
      "source": [
        "# Dự Đoán các ảnh trên tập test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np3O0GgN4s3n"
      },
      "source": [
        "## Tạo và load model đã lưu trước đó"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnnCTqhBT6Mq"
      },
      "source": [
        "# Load best model\n",
        "model = CNN(11)\n",
        "\n",
        "# Thiết lập kích thước input cho model\n",
        "dummy_x = tf.zeros((1, 224, 224, 3))\n",
        "model._set_inputs(dummy_x)\n",
        "\n",
        "# Load model đã lưu trước đó trong quá trình huấn luyện\n",
        "model.load_weights('my_model.h5')\n",
        "print(\"Model đã được load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaMQvDMJ46Vs"
      },
      "source": [
        "## Dự đoán nhãn của các ảnh trên tập test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLaGW2b55mqW"
      },
      "source": [
        "Sử dụng hàm predict để dự đoán"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBl_-M0_T6Mt"
      },
      "source": [
        "pred = model.predict(test_x)\n",
        "\n",
        "# pred là một ma trận xác suất của ảnh trên các lớp.\n",
        "# Ta lấy lớp có xác suất cao nhất trên từng ảnh bằng hàm argmax\n",
        "pred_labels = np.argmax(pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqXMbQte5o7U"
      },
      "source": [
        "Hiển thị thử kết quả của tập test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhVAFRO3T6Nn"
      },
      "source": [
        "test_df['label'] = predictions\n",
        "test_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlJ9T6eN5u9n"
      },
      "source": [
        "Lưu kết quả thành file CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx7VJJnCozqu"
      },
      "source": [
        "test_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLo6SJb-AjWD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UJBk2YToUAW"
      },
      "source": [
        "# Baseline model 2: Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udb21Jh3o0Jv"
      },
      "source": [
        "Ở baseline này, để cải thiện kết quả trên tập test, chúng ta có thể áp dụng kỹ thuật Image Augmentation. Thư viện Keras hỗ trợ class `ImageDataGenerator` trong việc augmentation. Ở ví dụ bên dưới, các kỹ thuật được áp dụng trong quá trình xử lý ảnh là:\n",
        "- rescale: đưa ảnh về miền giá trị [0,1]\n",
        "- rotation: xoay ảnh ngẫu nhiên trái phải 20 độ\n",
        "- shift: dịch chuyển ảnh sang trái phải, trên dưới 20% kích thước\n",
        "- horizontal flip: lật ảnh ngang ngẫu nhiên\n",
        "\n",
        "Tuy nhiên các kĩ thuật này chỉ áp dụng trong khi training, khi validation và testing, chỉ có rescale được áp dụng để tránh việc các kết quả validation không chính xác do data thay đổi qua các lần validate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bQA6Xz2thtu"
      },
      "source": [
        "# Data Generator dùng cho training\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Data Generator dùng cho validation và testing\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDGDU84TqDgU"
      },
      "source": [
        "Class `ImageDataGenerator` hỗ trợ đọc ảnh và augmentation realtime khi training, điều này giúp cho quá trình training không tốn nhiều bộ nhớ do không cần phải load toàn bộ ảnh như ban đầu. Ta có thể sử dụng hàm `flow_from_dataframe` để tạo luồng input từ DataFrame và folder chứa ảnh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSYU4w2Dt73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b76091e-6f3a-468f-b3d1-c6328ac420ce"
      },
      "source": [
        "batch_size = 24\n",
        "\n",
        "# ImageDataGenerator chỉ chấp nhận kiểu label là string, \n",
        "# ta chuyển cột label sang string\n",
        "train_df[\"label\"] = train_df[\"label\"].map(lambda x: str(x))\n",
        "# Chia dữ liệu thành train và validation\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.25)\n",
        "\n",
        "# Tạo luồng dữ liệu cho quá trình train, các bạn có thể tìm hiểu các tham số ở Keras document\n",
        "train_gen = train_datagen.flow_from_dataframe(dataframe=train_data, \n",
        "                                        directory=\"data/images\", \n",
        "                                        x_col=\"image\", \n",
        "                                        y_col=\"label\",\n",
        "                                        class_mode=\"categorical\",\n",
        "                                        target_size=(224,224), \n",
        "                                        batch_size=batch_size)\n",
        "\n",
        "# Tạo luồng dữ liệu cho quá trình test\n",
        "val_gen = test_datagen.flow_from_dataframe(dataframe=val_data, \n",
        "                                        directory=\"data/images\", \n",
        "                                        x_col=\"image\", \n",
        "                                        y_col=\"label\",\n",
        "                                        class_mode=\"categorical\",\n",
        "                                        shuffle=False,\n",
        "                                        target_size=(224,224), \n",
        "                                        batch_size=batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6175 validated image filenames belonging to 11 classes.\n",
            "Found 2059 validated image filenames belonging to 11 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl1UAMp8q51Z"
      },
      "source": [
        "Tiến hành huấn luyện"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usWQsXR3q0b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ea025a-d679-47c2-ce1e-fe5d3e052ae1"
      },
      "source": [
        "device = '/gpu:0' \n",
        "epochs = 50\n",
        "\n",
        "# Số lượng training step mỗi epoch\n",
        "steps_per_epoch = train_gen.n // batch_size\n",
        "# Số lượng validation step\n",
        "validation_steps = val_gen.n // batch_size\n",
        "\n",
        "with tf.device(device):\n",
        "    # Khởi tạo model\n",
        "    model = CNN(num_classes=11)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Tạo callback để lưu model có accuracy trên tập validation tốt nhất\n",
        "    mcp = tf.keras.callbacks.ModelCheckpoint(\"cnn_augmentation.h5\", monitor=\"val_accuracy\",\n",
        "                      save_best_only=True, save_weights_only=True)\n",
        "    rlr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, mode='max', patience=5, min_lr=1e-8, verbose=1)\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # Huấn luyện với data generator\n",
        "    model.fit(train_gen, \n",
        "                        steps_per_epoch=steps_per_epoch, \n",
        "                        epochs=epochs,\n",
        "                        verbose=1, \n",
        "                        validation_data=val_gen,\n",
        "                        validation_steps=validation_steps, \n",
        "                        callbacks=[mcp,rlr])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "257/257 [==============================] - 100s 384ms/step - loss: 3.0856 - accuracy: 0.2977 - val_loss: 2.7222 - val_accuracy: 0.2118\n",
            "Epoch 2/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 2.1179 - accuracy: 0.4170 - val_loss: 1.4239 - val_accuracy: 0.5750\n",
            "Epoch 3/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 1.4684 - accuracy: 0.5384 - val_loss: 1.3925 - val_accuracy: 0.5613\n",
            "Epoch 4/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 1.1621 - accuracy: 0.6123 - val_loss: 0.9425 - val_accuracy: 0.7005\n",
            "Epoch 5/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.9697 - accuracy: 0.6778 - val_loss: 1.0901 - val_accuracy: 0.6475\n",
            "Epoch 6/50\n",
            "257/257 [==============================] - 99s 384ms/step - loss: 0.8711 - accuracy: 0.7083 - val_loss: 0.9243 - val_accuracy: 0.7098\n",
            "Epoch 7/50\n",
            "257/257 [==============================] - 99s 387ms/step - loss: 0.7830 - accuracy: 0.7402 - val_loss: 1.5625 - val_accuracy: 0.5755\n",
            "Epoch 8/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.7571 - accuracy: 0.7501 - val_loss: 0.6700 - val_accuracy: 0.8000\n",
            "Epoch 9/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.6744 - accuracy: 0.7782 - val_loss: 0.8509 - val_accuracy: 0.7515\n",
            "Epoch 10/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.6277 - accuracy: 0.7934 - val_loss: 0.7855 - val_accuracy: 0.7583\n",
            "Epoch 11/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.5674 - accuracy: 0.8096 - val_loss: 0.6468 - val_accuracy: 0.8167\n",
            "Epoch 12/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.5614 - accuracy: 0.8130 - val_loss: 1.1108 - val_accuracy: 0.6941\n",
            "Epoch 13/50\n",
            "257/257 [==============================] - 99s 384ms/step - loss: 0.5164 - accuracy: 0.8325 - val_loss: 0.6951 - val_accuracy: 0.7794\n",
            "Epoch 14/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.5017 - accuracy: 0.8316 - val_loss: 0.4983 - val_accuracy: 0.8368\n",
            "Epoch 15/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.4943 - accuracy: 0.8321 - val_loss: 1.2171 - val_accuracy: 0.7039\n",
            "Epoch 16/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.4450 - accuracy: 0.8480 - val_loss: 0.5508 - val_accuracy: 0.8270\n",
            "Epoch 17/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.4104 - accuracy: 0.8621 - val_loss: 0.5759 - val_accuracy: 0.8265\n",
            "Epoch 18/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.4049 - accuracy: 0.8628 - val_loss: 0.9000 - val_accuracy: 0.7775\n",
            "Epoch 19/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.3859 - accuracy: 0.8730 - val_loss: 0.4512 - val_accuracy: 0.8583\n",
            "Epoch 20/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.3651 - accuracy: 0.8773 - val_loss: 0.5518 - val_accuracy: 0.8441\n",
            "Epoch 21/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.3709 - accuracy: 0.8795 - val_loss: 0.7697 - val_accuracy: 0.8049\n",
            "Epoch 22/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.3455 - accuracy: 0.8836 - val_loss: 0.5999 - val_accuracy: 0.8123\n",
            "Epoch 23/50\n",
            "257/257 [==============================] - 100s 388ms/step - loss: 0.3239 - accuracy: 0.8896 - val_loss: 0.4588 - val_accuracy: 0.8475\n",
            "Epoch 24/50\n",
            "257/257 [==============================] - 98s 383ms/step - loss: 0.2930 - accuracy: 0.8986 - val_loss: 0.6039 - val_accuracy: 0.8368\n",
            "Epoch 25/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.3009 - accuracy: 0.8945 - val_loss: 0.5237 - val_accuracy: 0.8623\n",
            "Epoch 26/50\n",
            "257/257 [==============================] - 99s 384ms/step - loss: 0.2908 - accuracy: 0.8994 - val_loss: 0.5994 - val_accuracy: 0.8456\n",
            "Epoch 27/50\n",
            "257/257 [==============================] - 99s 384ms/step - loss: 0.2949 - accuracy: 0.8987 - val_loss: 0.6620 - val_accuracy: 0.8069\n",
            "Epoch 28/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.2728 - accuracy: 0.9117 - val_loss: 0.6595 - val_accuracy: 0.8152\n",
            "Epoch 29/50\n",
            "257/257 [==============================] - 99s 383ms/step - loss: 0.2396 - accuracy: 0.9150 - val_loss: 0.3717 - val_accuracy: 0.8902\n",
            "Epoch 30/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.2367 - accuracy: 0.9216 - val_loss: 0.6342 - val_accuracy: 0.8245\n",
            "Epoch 31/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.2368 - accuracy: 0.9179 - val_loss: 0.5144 - val_accuracy: 0.8716\n",
            "Epoch 32/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.2380 - accuracy: 0.9197 - val_loss: 0.7130 - val_accuracy: 0.8083\n",
            "Epoch 33/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.2185 - accuracy: 0.9265 - val_loss: 0.4653 - val_accuracy: 0.8569\n",
            "Epoch 34/50\n",
            "257/257 [==============================] - 98s 383ms/step - loss: 0.1995 - accuracy: 0.9329 - val_loss: 0.3610 - val_accuracy: 0.9015\n",
            "Epoch 35/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.2157 - accuracy: 0.9239 - val_loss: 0.8027 - val_accuracy: 0.8137\n",
            "Epoch 36/50\n",
            "257/257 [==============================] - 99s 386ms/step - loss: 0.1894 - accuracy: 0.9332 - val_loss: 0.3378 - val_accuracy: 0.9186\n",
            "Epoch 37/50\n",
            "257/257 [==============================] - 98s 383ms/step - loss: 0.1932 - accuracy: 0.9333 - val_loss: 0.5021 - val_accuracy: 0.8750\n",
            "Epoch 38/50\n",
            "257/257 [==============================] - 98s 383ms/step - loss: 0.1810 - accuracy: 0.9343 - val_loss: 0.5172 - val_accuracy: 0.8892\n",
            "Epoch 39/50\n",
            "257/257 [==============================] - 98s 383ms/step - loss: 0.1822 - accuracy: 0.9395 - val_loss: 0.9486 - val_accuracy: 0.7755\n",
            "Epoch 40/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.1746 - accuracy: 0.9415 - val_loss: 0.4462 - val_accuracy: 0.8971\n",
            "Epoch 41/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.1655 - accuracy: 0.9415 - val_loss: 0.3903 - val_accuracy: 0.8892\n",
            "Epoch 42/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.1570 - accuracy: 0.9473 - val_loss: 0.4338 - val_accuracy: 0.8804\n",
            "Epoch 43/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.1630 - accuracy: 0.9457 - val_loss: 0.4369 - val_accuracy: 0.8922\n",
            "Epoch 44/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.1528 - accuracy: 0.9480 - val_loss: 0.5417 - val_accuracy: 0.8735\n",
            "Epoch 45/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.1520 - accuracy: 0.9498 - val_loss: 0.3348 - val_accuracy: 0.9132\n",
            "Epoch 46/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.1463 - accuracy: 0.9503 - val_loss: 0.3661 - val_accuracy: 0.9064\n",
            "Epoch 47/50\n",
            "257/257 [==============================] - 98s 381ms/step - loss: 0.1398 - accuracy: 0.9533 - val_loss: 0.4054 - val_accuracy: 0.9044\n",
            "Epoch 48/50\n",
            "257/257 [==============================] - 98s 382ms/step - loss: 0.1373 - accuracy: 0.9507 - val_loss: 0.5473 - val_accuracy: 0.8696\n",
            "Epoch 49/50\n",
            "257/257 [==============================] - 98s 380ms/step - loss: 0.1315 - accuracy: 0.9579 - val_loss: 0.5903 - val_accuracy: 0.8730\n",
            "Epoch 50/50\n",
            "257/257 [==============================] - 99s 386ms/step - loss: 0.1230 - accuracy: 0.9571 - val_loss: 0.3661 - val_accuracy: 0.9059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "XfJO3uZEGNF8",
        "outputId": "e6bc8c56-663f-4725-c7c8-2d6463e880d9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history = model.history\n",
        "loss = history .history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "miKsgQtAHE-9",
        "outputId": "042a8cb7-1f42-4ef6-ea37-cbb630c95519"
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "acc = history .history['accuracy']\n",
        "val_acc = history .history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VW8rt8VrkC7"
      },
      "source": [
        "Để predict trên tập test, ta cũng tạo một data generator với `class_mode` là `None`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTWHpPVyQR3C"
      },
      "source": [
        "test_gen = test_datagen.flow_from_dataframe(dataframe=test_df, \n",
        "                                        directory=\"data/images\", \n",
        "                                        x_col=\"image\",\n",
        "                                        class_mode=None,\n",
        "                                        shuffle=False,\n",
        "                                        batch_size=32,\n",
        "                                        target_size=(224,224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaNHbhX6r1SQ"
      },
      "source": [
        "Lưu ý, thứ tự các lớp sẽ không tuân theo quy luật từ 0-10 mà sẽ theo thứ tự alphabet khi sử dụng ImageDataGenerator. Do đó ta cần ánh xạ các indices sau khi predict về lớp tương ứng của nó:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVb_cZ-Aoj5n"
      },
      "source": [
        "pred = model.predict_generator(test_gen)\n",
        "\n",
        "# pred là một ma trận xác suất của ảnh trên các lớp.\n",
        "# Ta lấy lớp có xác suất cao nhất trên từng ảnh bằng hàm argmax\n",
        "predicted_class_indices = np.argmax(pred, axis=1)\n",
        "\n",
        "# Dictionary ánh xạ nhãn và index\n",
        "labels = train_gen.class_indices\n",
        "\n",
        "# Ánh xạ indices về nhãn đúng\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBTLU_fwswBx"
      },
      "source": [
        "# Baseline model 3: Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOQNzAFOp9qz"
      },
      "source": [
        "# from effiecientnet improt EfficientNetB3\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diBHD1ShNxL6"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYqRYl5JIJm7"
      },
      "source": [
        "# Data Generator dùng cho training\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Data Generator dùng cho validation và testing\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDRnVgjiILH7",
        "outputId": "b6691046-a8b2-4005-bfcc-4ec120dedb24"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "# ImageDataGenerator chỉ chấp nhận kiểu label là string, \n",
        "# ta chuyển cột label sang string\n",
        "train_df[\"label\"] = train_df[\"label\"].map(lambda x: str(x))\n",
        "# Chia dữ liệu thành train và validation\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.25)\n",
        "\n",
        "# Tạo luồng dữ liệu cho quá trình train, các bạn có thể tìm hiểu các tham số ở Keras document\n",
        "train_gen = train_datagen.flow_from_dataframe(dataframe=train_data, \n",
        "                                        directory=\"data/images\", \n",
        "                                        x_col=\"image\", \n",
        "                                        y_col=\"label\",\n",
        "                                        class_mode=\"categorical\",\n",
        "                                        target_size=(224,224), \n",
        "                                        batch_size=batch_size)\n",
        "\n",
        "# Tạo luồng dữ liệu cho quá trình test\n",
        "val_gen = test_datagen.flow_from_dataframe(dataframe=val_data, \n",
        "                                        directory=\"data/images\", \n",
        "                                        x_col=\"image\", \n",
        "                                        y_col=\"label\",\n",
        "                                        class_mode=\"categorical\",\n",
        "                                        shuffle=False,\n",
        "                                        target_size=(224,224), \n",
        "                                        batch_size=batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6175 validated image filenames belonging to 11 classes.\n",
            "Found 2059 validated image filenames belonging to 11 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM4TR8rlqB2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83adbf37-13c3-4a1b-e4e8-522aae7b97aa"
      },
      "source": [
        "# Số lượng training step mỗi epoch\n",
        "steps_per_epoch = train_gen.n // batch_size\n",
        "# Số lượng validation step\n",
        "validation_steps = val_gen.n // batch_size\n",
        "\n",
        "# Khởi tạo base model Resnet với pretrained weights từ ImageNet\n",
        "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Tạo model hoàn chỉnh bằng cách thêm lớp classifier\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "predictions = tf.keras.layers.Dense(11, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs = base_model.input, outputs = predictions)\n",
        "\n",
        "# Freeze các lớp CNN ban đầu\n",
        "base_model.trainable = False\n",
        "\n",
        "# Tạo callback để lưu model có accuracy trên tập validation tốt nhất\n",
        "mcp = tf.keras.callbacks.ModelCheckpoint(\"efficientnetb3.h5\", monitor=\"val_f1\",\n",
        "                  save_best_only=True, mode='max', save_weights_only=True, verbose=1)\n",
        "rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.1, mode='max', patience=5, min_lr=1e-8, verbose=1)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='categorical_crossentropy',\n",
        "              metrics=[f1])\n",
        "\n",
        "# model.load_weights(\"efficientnetb3.h5\")\n",
        "\n",
        "# Huấn luyện 10 epochs với learning rate lớn\n",
        "model.fit(train_gen, \n",
        "                    steps_per_epoch=steps_per_epoch, \n",
        "                    epochs=10,\n",
        "                    verbose=1, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=validation_steps, \n",
        "                    callbacks=[mcp,rlr])\n",
        "\n",
        "# Unfreeze toàn bộ mạng\n",
        "base_model.trainable = True\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy',\n",
        "              metrics=[f1])\n",
        "\n",
        "\n",
        "\n",
        "# Huấn luyện 100 epochs với learning rate nhỏ\n",
        "model.fit(train_gen, \n",
        "                    steps_per_epoch=steps_per_epoch, \n",
        "                    epochs=50,\n",
        "                    verbose=1, \n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=validation_steps, \n",
        "                    callbacks=[mcp])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "192/192 [==============================] - 105s 503ms/step - loss: 0.6081 - f1: 0.8087 - val_loss: 0.3072 - val_f1: 0.9039\n",
            "\n",
            "Epoch 00001: val_f1 improved from -inf to 0.90387, saving model to efficientnetb3.h5\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 95s 492ms/step - loss: 0.4183 - f1: 0.8722 - val_loss: 0.3448 - val_f1: 0.8989\n",
            "\n",
            "Epoch 00002: val_f1 did not improve from 0.90387\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 95s 492ms/step - loss: 0.3561 - f1: 0.8947 - val_loss: 0.2880 - val_f1: 0.9247\n",
            "\n",
            "Epoch 00003: val_f1 improved from 0.90387 to 0.92471, saving model to efficientnetb3.h5\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 95s 493ms/step - loss: 0.3369 - f1: 0.9012 - val_loss: 0.2542 - val_f1: 0.9236\n",
            "\n",
            "Epoch 00004: val_f1 did not improve from 0.92471\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 95s 493ms/step - loss: 0.2943 - f1: 0.9126 - val_loss: 0.2918 - val_f1: 0.9179\n",
            "\n",
            "Epoch 00005: val_f1 did not improve from 0.92471\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 94s 490ms/step - loss: 0.3311 - f1: 0.9045 - val_loss: 0.3017 - val_f1: 0.9185\n",
            "\n",
            "Epoch 00006: val_f1 did not improve from 0.92471\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 94s 491ms/step - loss: 0.2757 - f1: 0.9139 - val_loss: 0.2742 - val_f1: 0.9285\n",
            "\n",
            "Epoch 00007: val_f1 improved from 0.92471 to 0.92852, saving model to efficientnetb3.h5\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 94s 492ms/step - loss: 0.2921 - f1: 0.9124 - val_loss: 0.2956 - val_f1: 0.9214\n",
            "\n",
            "Epoch 00008: val_f1 did not improve from 0.92852\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 94s 491ms/step - loss: 0.2851 - f1: 0.9194 - val_loss: 0.2639 - val_f1: 0.9259\n",
            "\n",
            "Epoch 00009: val_f1 did not improve from 0.92852\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 94s 490ms/step - loss: 0.2837 - f1: 0.9226 - val_loss: 0.3661 - val_f1: 0.9194\n",
            "\n",
            "Epoch 00010: val_f1 did not improve from 0.92852\n",
            "Epoch 1/50\n",
            "192/192 [==============================] - 144s 684ms/step - loss: 1.9013 - f1: 0.6152 - val_loss: 0.6863 - val_f1: 0.8410\n",
            "\n",
            "Epoch 00001: val_f1 did not improve from 0.92852\n",
            "Epoch 2/50\n",
            "192/192 [==============================] - 130s 676ms/step - loss: 1.2543 - f1: 0.7210 - val_loss: 0.7177 - val_f1: 0.8292\n",
            "\n",
            "Epoch 00002: val_f1 did not improve from 0.92852\n",
            "Epoch 3/50\n",
            "192/192 [==============================] - 129s 670ms/step - loss: 0.9680 - f1: 0.7763 - val_loss: 0.6199 - val_f1: 0.8510\n",
            "\n",
            "Epoch 00003: val_f1 did not improve from 0.92852\n",
            "Epoch 4/50\n",
            "192/192 [==============================] - 129s 670ms/step - loss: 0.7901 - f1: 0.8032 - val_loss: 0.5348 - val_f1: 0.8623\n",
            "\n",
            "Epoch 00004: val_f1 did not improve from 0.92852\n",
            "Epoch 5/50\n",
            "192/192 [==============================] - 129s 667ms/step - loss: 0.6406 - f1: 0.8343 - val_loss: 0.4684 - val_f1: 0.8766\n",
            "\n",
            "Epoch 00005: val_f1 did not improve from 0.92852\n",
            "Epoch 6/50\n",
            "192/192 [==============================] - 129s 668ms/step - loss: 0.5516 - f1: 0.8560 - val_loss: 0.4223 - val_f1: 0.8863\n",
            "\n",
            "Epoch 00006: val_f1 did not improve from 0.92852\n",
            "Epoch 7/50\n",
            "192/192 [==============================] - 128s 667ms/step - loss: 0.4814 - f1: 0.8725 - val_loss: 0.3942 - val_f1: 0.8983\n",
            "\n",
            "Epoch 00007: val_f1 did not improve from 0.92852\n",
            "Epoch 8/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.3978 - f1: 0.8861 - val_loss: 0.3645 - val_f1: 0.9066\n",
            "\n",
            "Epoch 00008: val_f1 did not improve from 0.92852\n",
            "Epoch 9/50\n",
            "192/192 [==============================] - 128s 664ms/step - loss: 0.3689 - f1: 0.8962 - val_loss: 0.3423 - val_f1: 0.9134\n",
            "\n",
            "Epoch 00009: val_f1 did not improve from 0.92852\n",
            "Epoch 10/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.3325 - f1: 0.9021 - val_loss: 0.3295 - val_f1: 0.9159\n",
            "\n",
            "Epoch 00010: val_f1 did not improve from 0.92852\n",
            "Epoch 11/50\n",
            "192/192 [==============================] - 128s 663ms/step - loss: 0.3034 - f1: 0.9102 - val_loss: 0.3171 - val_f1: 0.9185\n",
            "\n",
            "Epoch 00011: val_f1 did not improve from 0.92852\n",
            "Epoch 12/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.2629 - f1: 0.9192 - val_loss: 0.3037 - val_f1: 0.9230\n",
            "\n",
            "Epoch 00012: val_f1 did not improve from 0.92852\n",
            "Epoch 13/50\n",
            "192/192 [==============================] - 128s 664ms/step - loss: 0.2530 - f1: 0.9247 - val_loss: 0.2931 - val_f1: 0.9271\n",
            "\n",
            "Epoch 00013: val_f1 did not improve from 0.92852\n",
            "Epoch 14/50\n",
            "192/192 [==============================] - 128s 664ms/step - loss: 0.2312 - f1: 0.9271 - val_loss: 0.2852 - val_f1: 0.9276\n",
            "\n",
            "Epoch 00014: val_f1 did not improve from 0.92852\n",
            "Epoch 15/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.2241 - f1: 0.9312 - val_loss: 0.2734 - val_f1: 0.9337\n",
            "\n",
            "Epoch 00015: val_f1 improved from 0.92852 to 0.93375, saving model to efficientnetb3.h5\n",
            "Epoch 16/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.1995 - f1: 0.9367 - val_loss: 0.2649 - val_f1: 0.9323\n",
            "\n",
            "Epoch 00016: val_f1 did not improve from 0.93375\n",
            "Epoch 17/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.1827 - f1: 0.9400 - val_loss: 0.2543 - val_f1: 0.9392\n",
            "\n",
            "Epoch 00017: val_f1 improved from 0.93375 to 0.93916, saving model to efficientnetb3.h5\n",
            "Epoch 18/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.1898 - f1: 0.9411 - val_loss: 0.2490 - val_f1: 0.9387\n",
            "\n",
            "Epoch 00018: val_f1 did not improve from 0.93916\n",
            "Epoch 19/50\n",
            "192/192 [==============================] - 128s 664ms/step - loss: 0.1677 - f1: 0.9492 - val_loss: 0.2404 - val_f1: 0.9417\n",
            "\n",
            "Epoch 00019: val_f1 improved from 0.93916 to 0.94165, saving model to efficientnetb3.h5\n",
            "Epoch 20/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.1464 - f1: 0.9555 - val_loss: 0.2346 - val_f1: 0.9414\n",
            "\n",
            "Epoch 00020: val_f1 did not improve from 0.94165\n",
            "Epoch 21/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.1492 - f1: 0.9530 - val_loss: 0.2310 - val_f1: 0.9414\n",
            "\n",
            "Epoch 00021: val_f1 did not improve from 0.94165\n",
            "Epoch 22/50\n",
            "192/192 [==============================] - 129s 668ms/step - loss: 0.1367 - f1: 0.9559 - val_loss: 0.2276 - val_f1: 0.9436\n",
            "\n",
            "Epoch 00022: val_f1 improved from 0.94165 to 0.94362, saving model to efficientnetb3.h5\n",
            "Epoch 23/50\n",
            "192/192 [==============================] - 128s 667ms/step - loss: 0.1183 - f1: 0.9594 - val_loss: 0.2254 - val_f1: 0.9465\n",
            "\n",
            "Epoch 00023: val_f1 improved from 0.94362 to 0.94654, saving model to efficientnetb3.h5\n",
            "Epoch 24/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.1204 - f1: 0.9602 - val_loss: 0.2201 - val_f1: 0.9456\n",
            "\n",
            "Epoch 00024: val_f1 did not improve from 0.94654\n",
            "Epoch 25/50\n",
            "192/192 [==============================] - 128s 667ms/step - loss: 0.1132 - f1: 0.9648 - val_loss: 0.2125 - val_f1: 0.9483\n",
            "\n",
            "Epoch 00025: val_f1 improved from 0.94654 to 0.94831, saving model to efficientnetb3.h5\n",
            "Epoch 26/50\n",
            "192/192 [==============================] - 129s 670ms/step - loss: 0.0945 - f1: 0.9675 - val_loss: 0.2111 - val_f1: 0.9498\n",
            "\n",
            "Epoch 00026: val_f1 improved from 0.94831 to 0.94985, saving model to efficientnetb3.h5\n",
            "Epoch 27/50\n",
            "192/192 [==============================] - 131s 681ms/step - loss: 0.0992 - f1: 0.9680 - val_loss: 0.2099 - val_f1: 0.9491\n",
            "\n",
            "Epoch 00027: val_f1 did not improve from 0.94985\n",
            "Epoch 28/50\n",
            "192/192 [==============================] - 129s 670ms/step - loss: 0.0951 - f1: 0.9688 - val_loss: 0.2053 - val_f1: 0.9518\n",
            "\n",
            "Epoch 00028: val_f1 improved from 0.94985 to 0.95176, saving model to efficientnetb3.h5\n",
            "Epoch 29/50\n",
            "192/192 [==============================] - 129s 672ms/step - loss: 0.0778 - f1: 0.9748 - val_loss: 0.2074 - val_f1: 0.9513\n",
            "\n",
            "Epoch 00029: val_f1 did not improve from 0.95176\n",
            "Epoch 30/50\n",
            "192/192 [==============================] - 129s 671ms/step - loss: 0.0963 - f1: 0.9693 - val_loss: 0.2120 - val_f1: 0.9459\n",
            "\n",
            "Epoch 00030: val_f1 did not improve from 0.95176\n",
            "Epoch 31/50\n",
            "192/192 [==============================] - 130s 673ms/step - loss: 0.0682 - f1: 0.9761 - val_loss: 0.2089 - val_f1: 0.9466\n",
            "\n",
            "Epoch 00031: val_f1 did not improve from 0.95176\n",
            "Epoch 32/50\n",
            "192/192 [==============================] - 129s 669ms/step - loss: 0.0837 - f1: 0.9718 - val_loss: 0.2046 - val_f1: 0.9493\n",
            "\n",
            "Epoch 00032: val_f1 did not improve from 0.95176\n",
            "Epoch 33/50\n",
            "192/192 [==============================] - 129s 669ms/step - loss: 0.0697 - f1: 0.9759 - val_loss: 0.2012 - val_f1: 0.9484\n",
            "\n",
            "Epoch 00033: val_f1 did not improve from 0.95176\n",
            "Epoch 34/50\n",
            "192/192 [==============================] - 129s 669ms/step - loss: 0.0721 - f1: 0.9747 - val_loss: 0.1969 - val_f1: 0.9486\n",
            "\n",
            "Epoch 00034: val_f1 did not improve from 0.95176\n",
            "Epoch 35/50\n",
            "192/192 [==============================] - 129s 671ms/step - loss: 0.0592 - f1: 0.9773 - val_loss: 0.1945 - val_f1: 0.9496\n",
            "\n",
            "Epoch 00035: val_f1 did not improve from 0.95176\n",
            "Epoch 36/50\n",
            "192/192 [==============================] - 128s 665ms/step - loss: 0.0574 - f1: 0.9786 - val_loss: 0.1962 - val_f1: 0.9531\n",
            "\n",
            "Epoch 00036: val_f1 improved from 0.95176 to 0.95306, saving model to efficientnetb3.h5\n",
            "Epoch 37/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.0585 - f1: 0.9794 - val_loss: 0.1929 - val_f1: 0.9528\n",
            "\n",
            "Epoch 00037: val_f1 did not improve from 0.95306\n",
            "Epoch 38/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.0596 - f1: 0.9786 - val_loss: 0.1875 - val_f1: 0.9542\n",
            "\n",
            "Epoch 00038: val_f1 improved from 0.95306 to 0.95421, saving model to efficientnetb3.h5\n",
            "Epoch 39/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.0536 - f1: 0.9800 - val_loss: 0.1915 - val_f1: 0.9537\n",
            "\n",
            "Epoch 00039: val_f1 did not improve from 0.95421\n",
            "Epoch 40/50\n",
            "192/192 [==============================] - 128s 667ms/step - loss: 0.0495 - f1: 0.9824 - val_loss: 0.1874 - val_f1: 0.9545\n",
            "\n",
            "Epoch 00040: val_f1 improved from 0.95421 to 0.95450, saving model to efficientnetb3.h5\n",
            "Epoch 41/50\n",
            "192/192 [==============================] - 128s 666ms/step - loss: 0.0571 - f1: 0.9799 - val_loss: 0.1827 - val_f1: 0.9570\n",
            "\n",
            "Epoch 00041: val_f1 improved from 0.95450 to 0.95696, saving model to efficientnetb3.h5\n",
            "Epoch 42/50\n",
            "192/192 [==============================] - 128s 667ms/step - loss: 0.0509 - f1: 0.9823 - val_loss: 0.1861 - val_f1: 0.9547\n",
            "\n",
            "Epoch 00042: val_f1 did not improve from 0.95696\n",
            "Epoch 43/50\n",
            "192/192 [==============================] - 128s 667ms/step - loss: 0.0499 - f1: 0.9808 - val_loss: 0.1796 - val_f1: 0.9543\n",
            "\n",
            "Epoch 00043: val_f1 did not improve from 0.95696\n",
            "Epoch 44/50\n",
            "192/192 [==============================] - 129s 670ms/step - loss: 0.0525 - f1: 0.9818 - val_loss: 0.1822 - val_f1: 0.9555\n",
            "\n",
            "Epoch 00044: val_f1 did not improve from 0.95696\n",
            "Epoch 45/50\n",
            "192/192 [==============================] - 131s 679ms/step - loss: 0.0522 - f1: 0.9804 - val_loss: 0.1836 - val_f1: 0.9567\n",
            "\n",
            "Epoch 00045: val_f1 did not improve from 0.95696\n",
            "Epoch 46/50\n",
            "192/192 [==============================] - 131s 678ms/step - loss: 0.0424 - f1: 0.9847 - val_loss: 0.1806 - val_f1: 0.9550\n",
            "\n",
            "Epoch 00046: val_f1 did not improve from 0.95696\n",
            "Epoch 47/50\n",
            "192/192 [==============================] - 131s 680ms/step - loss: 0.0472 - f1: 0.9838 - val_loss: 0.1803 - val_f1: 0.9557\n",
            "\n",
            "Epoch 00047: val_f1 did not improve from 0.95696\n",
            "Epoch 48/50\n",
            "192/192 [==============================] - 131s 680ms/step - loss: 0.0373 - f1: 0.9874 - val_loss: 0.1760 - val_f1: 0.9587\n",
            "\n",
            "Epoch 00048: val_f1 improved from 0.95696 to 0.95868, saving model to efficientnetb3.h5\n",
            "Epoch 49/50\n",
            "192/192 [==============================] - 131s 680ms/step - loss: 0.0432 - f1: 0.9851 - val_loss: 0.1765 - val_f1: 0.9582\n",
            "\n",
            "Epoch 00049: val_f1 did not improve from 0.95868\n",
            "Epoch 50/50\n",
            "192/192 [==============================] - 132s 684ms/step - loss: 0.0379 - f1: 0.9871 - val_loss: 0.1759 - val_f1: 0.9604\n",
            "\n",
            "Epoch 00050: val_f1 improved from 0.95868 to 0.96036, saving model to efficientnetb3.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f987e4d9e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqHSMra3gFZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854558ce-fdb3-475d-d6c8-33802a791858"
      },
      "source": [
        "# Số lượng training step mỗi epoch\n",
        "steps_per_epoch = train_gen.n // batch_size\n",
        "# Số lượng validation step\n",
        "validation_steps = val_gen.n // batch_size\n",
        "# Khởi tạo base model Resnet với pretrained weights từ ImageNet\n",
        "base_model = tf.keras.applications.ResNet50(input_shape=(224,224,3),\n",
        "                                               include_top=False, \n",
        "                                               weights='imagenet')\n",
        "    \n",
        "# Tạo model hoàn chỉnh bằng cách thêm lớp classifier\n",
        "model = tf.keras.Sequential([\n",
        "      base_model,\n",
        "      tf.keras.layers.GlobalAveragePooling2D(),\n",
        "      tf.keras.layers.Dense(11, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    # Freeze các lớp CNN ban đầu\n",
        "base_model.trainable = False\n",
        "    \n",
        "    # Tạo callback để lưu model có accuracy trên tập validation tốt nhất\n",
        "mcp = tf.keras.callbacks.ModelCheckpoint(\"resnet50.h5\", monitor=\"val_f1\",\n",
        "                      save_best_only=True, save_weights_only=True)\n",
        "rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.1, mode='max', \n",
        "                                               patience=5, min_lr=1e-8, verbose=1)\n",
        "    \n",
        "    # Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='categorical_crossentropy',\n",
        "                  metrics=[f1])\n",
        "    \n",
        "    # Huấn luyện 10 epochs với learning rate lớn\n",
        "model.fit(train_gen,steps_per_epoch=steps_per_epoch, \n",
        "                        epochs=10,\n",
        "                        verbose=1, \n",
        "                        validation_data=val_gen,\n",
        "                        validation_steps=validation_steps, \n",
        "                        callbacks=[mcp,rlr])\n",
        "    \n",
        "# Unfreeze toàn bộ mạng\n",
        "base_model.trainable = True\n",
        "    \n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy',\n",
        "                  metrics=[f1])\n",
        "    \n",
        "# Huấn luyện 100 epochs với learning rate nhỏ\n",
        "model.fit(train_gen,steps_per_epoch=steps_per_epoch, \n",
        "                        epochs=50,\n",
        "                        verbose=1, \n",
        "                        validation_data=val_gen,\n",
        "                        validation_steps=validation_steps, \n",
        "                        callbacks=[mcp])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "192/192 [==============================] - 1948s 10s/step - loss: 1.3988 - f1: 0.7094 - val_loss: 0.8266 - val_f1: 0.8420\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 94s 492ms/step - loss: 0.9159 - f1: 0.8071 - val_loss: 0.9520 - val_f1: 0.8445\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 93s 482ms/step - loss: 0.7130 - f1: 0.8487 - val_loss: 0.9783 - val_f1: 0.8411\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 92s 481ms/step - loss: 0.6551 - f1: 0.8646 - val_loss: 0.8003 - val_f1: 0.8899\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 92s 479ms/step - loss: 0.7624 - f1: 0.8595 - val_loss: 0.9626 - val_f1: 0.8755\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 92s 479ms/step - loss: 0.6828 - f1: 0.8730 - val_loss: 1.0335 - val_f1: 0.8691\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 92s 479ms/step - loss: 0.5796 - f1: 0.8923 - val_loss: 1.0992 - val_f1: 0.8752\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 92s 479ms/step - loss: 0.7057 - f1: 0.8755 - val_loss: 1.0949 - val_f1: 0.8812\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 92s 478ms/step - loss: 0.7308 - f1: 0.8824 - val_loss: 1.0708 - val_f1: 0.8795\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 92s 478ms/step - loss: 0.3884 - f1: 0.9259 - val_loss: 0.7621 - val_f1: 0.9126\n",
            "Epoch 1/50\n",
            "192/192 [==============================] - 116s 567ms/step - loss: 0.9723 - f1: 0.8625 - val_loss: 0.8116 - val_f1: 0.9077\n",
            "Epoch 2/50\n",
            "192/192 [==============================] - 108s 560ms/step - loss: 0.4638 - f1: 0.9171 - val_loss: 0.7087 - val_f1: 0.9162\n",
            "Epoch 3/50\n",
            "192/192 [==============================] - 108s 559ms/step - loss: 0.3109 - f1: 0.9375 - val_loss: 0.7445 - val_f1: 0.9098\n",
            "Epoch 4/50\n",
            "192/192 [==============================] - 107s 559ms/step - loss: 0.2385 - f1: 0.9480 - val_loss: 0.6351 - val_f1: 0.9218\n",
            "Epoch 5/50\n",
            "192/192 [==============================] - 108s 560ms/step - loss: 0.1643 - f1: 0.9577 - val_loss: 0.5974 - val_f1: 0.9283\n",
            "Epoch 6/50\n",
            "192/192 [==============================] - 109s 566ms/step - loss: 0.1620 - f1: 0.9634 - val_loss: 0.6156 - val_f1: 0.9254\n",
            "Epoch 7/50\n",
            "192/192 [==============================] - 108s 560ms/step - loss: 0.1385 - f1: 0.9655 - val_loss: 0.5927 - val_f1: 0.9284\n",
            "Epoch 8/50\n",
            "192/192 [==============================] - 107s 558ms/step - loss: 0.1229 - f1: 0.9695 - val_loss: 0.5533 - val_f1: 0.9318\n",
            "Epoch 9/50\n",
            "192/192 [==============================] - 108s 560ms/step - loss: 0.0828 - f1: 0.9760 - val_loss: 0.5414 - val_f1: 0.9330\n",
            "Epoch 10/50\n",
            "192/192 [==============================] - 107s 559ms/step - loss: 0.0799 - f1: 0.9776 - val_loss: 0.5461 - val_f1: 0.9335\n",
            "Epoch 11/50\n",
            "192/192 [==============================] - 108s 559ms/step - loss: 0.0977 - f1: 0.9763 - val_loss: 0.5310 - val_f1: 0.9331\n",
            "Epoch 12/50\n",
            "192/192 [==============================] - 107s 559ms/step - loss: 0.0722 - f1: 0.9809 - val_loss: 0.5461 - val_f1: 0.9338\n",
            "Epoch 13/50\n",
            "188/192 [============================>.] - ETA: 2s - loss: 0.0738 - f1: 0.9820"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlYA3IwQD4tk",
        "outputId": "6c80eec7-5249-4049-8a87-f3fc2ff8b2b6"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print('Current Device: ', tf.test.gpu_device_name() + '\\n')\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Device:  /device:GPU:0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 1951670194529485588, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14509932544\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 3114433218443238655\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPaZVmoEuSBQ"
      },
      "source": [
        "# Các phương pháp khác\n",
        "Ngoài các baseline models được giới thiệu ở trên, có thể sử dụng thêm các phương pháp khác như:\n",
        "- Sử dụng thư viện augmentation phức tạo hơn `imgaug`.\n",
        "- Oversampling để làm tăng độ cân bằng của dữ liệu.\n",
        "- Thêm weights vào hàm loss để làm tăng độ cân bằng giữa các nhãn.\n",
        "- Sử dụng các kiến trúc CNN khác, hoặc kết hợp nhiều kiến trúc lại với nhau.\n",
        "- Sử dụng ten-crop validation khi testing.\n",
        "- Sử dụng các phương pháp cross-validation để chia dữ liệu khi huấn luyện."
      ]
    }
  ]
}